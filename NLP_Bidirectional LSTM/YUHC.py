# -*- coding: utf-8 -*-
"""HW4_YuhengChen.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X1vAqVMo-D_i-3ME1IRAXUJCfcY_NyQ5
"""

# from google.colab import drive
# drive.mount('/content/gdrive')

import sys

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
import numpy as np
import gzip
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence
from torch.utils.data import TensorDataset, DataLoader
from torch.nn.utils.rnn import pad_sequence

input_f = sys.argv[1]
input_v = sys.argv[2]
input_t = sys.argv[3]
save_path = sys.argv[4]
glove_file = sys.argv[5]

import warnings

warnings.filterwarnings("ignore")

f = open(input_f, "r")
train_text = [line.strip() for line in f.readlines()]

f2 = open(input_v, "r")
val_text = [line.strip() for line in f2.readlines()]

f3 = open(input_t, "r")
test_text = [line.strip() for line in f3.readlines()]

# len(test_text)

split = [item.split(' ') for item in train_text] 
## create a dictionary to count word occurences
dict1 = {}
for item in split:
    if len(item) > 1:
        key = item[1]
        if key not in dict1:
            dict1[key] = 1
        else:
            dict1[key] += 1
# print(dict1['-DOCSTART-'])
## sort dictionary in descending order
dict_sorted = {k: v for k, v in sorted(dict1.items(), key=lambda item: item[1], reverse = True)}
## replace rare words whose occurence less than 3 with a special token ‘< unk >’.
rare_words_occ = 0
rare_words = []
for k,v in dict_sorted.items():
    if v < 2:
        rare_words_occ += v
        rare_words.append([k,v])

## delete rare words and add unkown token to vocabulary and update vocabulary
for item in rare_words:
      dict_sorted.pop(item[0])
updict = {'<unk>': rare_words_occ}
dict_vocab = {**updict, **dict_sorted}

## create word2idx
i = 1
for key, value in dict_vocab.items():
  dict_vocab[key] = i
  i += 1



"""## Task 1: Simple Bidirectional LSTM model"""

def read_data(file_path):
    sentences = []
    tags = []
    with open(file_path, 'r') as f:
        sentence = []
        tag = []

        for line in f:
            if line.strip():
                # word, _, ner_tag = line.strip().split()
                index, word, ner_tag = line.strip().split()
                # word = word.lower()
                sentence.append(word)
                tag.append(ner_tag)
            else:
                sentences.append(sentence)
                tags.append(tag)
                sentence = []
                tag = []
        if sentence:
            sentences.append(sentence)
            tags.append(tag)
    return sentences, tags

def create_tag2id(tags):
    tag2id = {}
    for tag_list in tags:
        for tag in tag_list:
            if tag not in tag2id:
                tag2id[tag] = len(tag2id)
    return tag2id

emb_dim = 100
hidden_dim = 256
drop_out = 0.33
output_dim = 128
# vocab_size = len(word2id)
batch_size = 1
sentences, tags_all = read_data(input_f)
tags_idx = create_tag2id(tags_all)
# ## create dataset for training 
# sentences, tags = read_data(input_f)
# word2id = create_vocab(sentences)
# vocab_size = len(word2id)
tag2id = create_tag2id(tags_all)

# tag2id

sentences_dev, tags_dev = read_data(input_v)

def read_test_data(file_path):
    sentences = []
    with open(file_path, 'r') as f:
        sentence = []

        for line in f:
            if line.strip():
                # word, _, ner_tag = line.strip().split()
                index, word = line.strip().split()
                # word = word.lower()
                sentence.append(word)
                
            else:
                sentences.append(sentence)
                
                sentence = []
                
        if sentence:
            sentences.append(sentence)
            
    return sentences

test_sentence = read_test_data(input_t)

def create_test_dataset(sen):
  text = []


  for sentence in sen:
    sentence_text = []
    for item in sentence:
      if item in dict_vocab:
        embedding = torch.tensor(dict_vocab[item])
      else:
        embedding = torch.tensor(dict_vocab['<unk>'])
      sentence_text.append(embedding)
    text.append(sentence_text)


  # print(text)
  text = [torch.tensor(i) for i in text]
  text = pad_sequence(text, batch_first = True, padding_value= 0) 


  # create a TensorDataset
  dataset = TensorDataset(text)
  # create the train_dataloader
  batch_size = 20
  dataloader = DataLoader(dataset, batch_size=20, shuffle=True)
  # print(tags)
  return dataloader, text

def create_dataset(sen, tag):
  text = []
  text_len = []
  tags = []

  for sentence in sen:
    sentence_text = []
    for item in sentence:
      if item in dict_vocab:
        embedding = torch.tensor(dict_vocab[item])
      else:
        embedding = torch.tensor(dict_vocab['<unk>'])
      sentence_text.append(embedding)
    text.append(sentence_text)
    text_len.append(len(sentence_text))

  # print(len(tag))
  for taglst in tag:
    # print(taglst)
    sentence_tag = []
    for item in taglst:
      # print(item)
      sentence_tag.append(torch.tensor(tag2id[item]))
    tags.append(sentence_tag)
  # print(text)
  text = [torch.tensor(i) for i in text]
  text = pad_sequence(text, batch_first = True, padding_value= 0) 
  text_len = torch.tensor(text_len)
  tags = [torch.tensor(i) for i in tags]
  tags = pad_sequence(tags, batch_first = True, padding_value= 100)

  # print(text.size(), tags.size())
  # print(text)



  # create a TensorDataset
  dataset = TensorDataset(text, tags)
  # create the train_dataloader
  batch_size = 20
  dataloader = DataLoader(dataset, batch_size=20, shuffle=True)
  # print(tags)
  return dataloader, text, tags

train_dataloader, train_text, train_tags = create_dataset(sentences, tags_all)
dev_dataloader, dev_text, dev_tags = create_dataset(sentences_dev, tags_dev)
test_dataloader, test_text = create_test_dataset(test_sentence)
train_dataloader.batch_size

# text,tags = create_dataset(sentences, tags_all)
# dev_text, dev_tags = create_dataset(sentences_dev, tags_dev)
# train_d = Train(text, tags)
# dev_d = Train(dev_text, dev_tags)

# len(text[0])

from pandas.core import accessor
import torch
import torch.nn as nn
class NERModel(nn.Module):
    def __init__(self, num_embeddings, embedding_dim, hidden_dim, output_dim, dropout, num_layers):
        super().__init__()
        self.embedding = nn.Embedding(num_embeddings, embedding_dim)
        self.bilstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, 
                              bidirectional=True, batch_first=True)
        self.dropout = nn.Dropout(dropout)
        # self.linear = nn.Linear(hidden_dim * 2, output_dim)
        # self.elu = nn.ELU()
        # self.classifier = nn.Linear(output_dim, 9)
        self.linear = nn.Linear(hidden_dim*2, hidden_dim)
        self.activation = nn.ELU()
        self.classifier = nn.Linear(hidden_dim, output_dim)
        self.cl2 = nn.Linear(output_dim,9)


        
    def forward(self, x):
        out = self.embedding(x)
        out, _ = self.bilstm(out)      
        out = self.dropout(out)   
        out = self.linear(out)      
        out = self.activation(out)    
        out = self.classifier(out)  
        out = self.cl2(out)
        out = out.permute(0,2,1)

        
        return out


# # Instantiate the model
# # Define the loss function and optimizer
# from sklearn.metrics import f1_score
# embedding_dim = 100
# hidden_dim = 256

# num_layers = 1
# dropout = 0.33
# output_dim = 128
# lr = 0.1
# batch_size = 20
# epochs = 140
# # CUDA_LAUNCH_BLOCKING = "1"
# # torch.backends.cudnn.deterministic = True
# # Specify the device (GPU if available, otherwise CPU)
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# model = NERModel(num_embeddings=14987, embedding_dim=embedding_dim, 
#                  hidden_dim=hidden_dim, output_dim=output_dim, dropout = dropout,
#                  num_layers = num_layers)
# blstm1 = model.to(device)
# loss_function = nn.CrossEntropyLoss(ignore_index = 100)
# optimizer = torch.optim.SGD(blstm1.parameters(), lr=lr)

# # Train the model
# for epoch in range(epochs):
#     epoch_loss = 0.0
#     for batch_inputs, batch_labels in train_dataloader:
#         # Send the batch to the device
#         batch_inputs, batch_labels = batch_inputs.to(device), batch_labels.to(device)
#         optimizer.zero_grad()
#         outputs = blstm1(batch_inputs)
#         loss = loss_function(outputs, batch_labels)
#         loss.backward()
#         optimizer.step()
#         epoch_loss += loss.item()
#         pred = torch.argmax(outputs,axis=1)

#     epoch_loss /= len(train_dataloader)
#     # epoch_f1 /= len(train_dataloader)
#     print(f"Epoch {epoch+1}: Loss={epoch_loss:.4f}")
#     # , F1 Score={epoch_f1:.4f}"

# ## save model
save_path_blstm1 =  save_path+"/blstm1.pt"
# torch.save(blstm1, save_path_blstm1)

#load model

blstm1 = torch.load(save_path_blstm1, map_location=torch.device("cuda" if torch.cuda.is_available() else "cpu"))
print('model1')

# save_path_blstm1 = "gdrive/MyDrive/Colab Notebooks/HW4/blstm1.pt"
# blstm1 = torch.load(save_path_blstm1)

# y_pred = []
# y_true = []
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# blstm1.eval()
# with torch.no_grad():
#     for batch_inputs, batch_labels in dev_dataloader:           
#         batch_inputs = batch_inputs.to(device)
#         batch_labels = batch_labels.to(device)

#         output = blstm1(batch_inputs)

#         y_pred.extend(output.tolist())
#         y_true.extend(batch_labels.tolist())

## predictions for test data

y_pred_test = []
pop = []
final_predictions = []
ignore_idx = 100
# y_true = []
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
blstm1.eval()
test_dataset = TensorDataset(test_text)
test_d = DataLoader(test_dataset,shuffle=False)

with torch.no_grad():
    for batch_inputs in test_d:           
        batch_inputs = batch_inputs[0].to(device)

        output = blstm1(batch_inputs)

        y_pred_test.append(torch.argmax(output, dim=1))

for i in test_text:
    pop.append(i[i!=ignore_idx].tolist())
# print(len(y_pred_test), len(pop))

for i in range(len(pop)):
    
    final_predictions+=y_pred_test[i][0][:len(pop[i])].tolist()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
def predictions(dev_x, dev_y, ignore_idx, blstm1):
  true_tag = []
  position = []
  predictions = []
  final_pred = []
  for i in dev_y:
    true_tag+=i[i!=ignore_idx].tolist()
    position.append(i[i!=ignore_idx].tolist())

  dev_dataset = TensorDataset(dev_x)
  dev_d = DataLoader(dev_dataset,shuffle=False)

  blstm1.eval()
  with torch.no_grad():
    for inputs in dev_d:
      i = inputs[0].to(device)
        
      outputs = blstm1(i)
      predictions.append(torch.argmax(outputs, dim=1))
  # print(len(predictions), len(position))
  for i in range(len(position)):
    
    final_pred+=predictions[i][0][:len(position[i])].tolist()
  return true_tag, final_pred

from sklearn.metrics import f1_score
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
true_tag, pred = predictions(dev_text, dev_tags, 100, blstm1)
f1_sklearn = f1_score(true_tag, pred, average='macro')
print('F1 score in sklearn in model2 is', f1_sklearn)

id2tag = {y: x for x, y in tag2id.items()}
# id2tag


## add predictions to dev
f2 = open(input_v, "r")
val_text = [line.strip() for line in f2.readlines()]

j = 0
for i in range(len(val_text)):
  if val_text[i] == '':
    continue
  else:
    split_str = val_text[i].split(' ')
    # print(pred[j])
    split_str[2] = id2tag[pred[j]]
    val_text[i] = ' '.join(split_str)
    j += 1
    # item += '3'

f3 = open(input_t, "r")
test_text = [line.strip() for line in f3.readlines()]
## add predictions to test
j = 0 
for i in range(len(test_text)):
  if test_text[i] == '':
    continue
  else:
    test_text[i] += ' '
    test_text[i] += id2tag[final_predictions[j]]
    j += 1

## self-predicted file
f2 = open(input_v, "r")
val_text_self = [line.strip() for line in f2.readlines()]

j = 0
for i in range(len(val_text_self)):
  if val_text_self[i] == '':
    continue
  else:
    val_text_self[i] += ' '
    # print(pred[j])
    val_text_self[i] += id2tag[pred[j]]
    j += 1
    # item += '3'

print('output files for model 1')
file_test_pred = open('dev1.out','w')
for item in val_text:
	file_test_pred.write(item+"\n")
file_test_pred.close()

# file_test_pred_self = open('self_dev1.out','w')
# for item in val_text_self:
# 	file_test_pred_self.write(item+"\n")
# file_test_pred_self.close()

# file_test_pred_test = open('test1.out','w')
# for item in test_text:
# 	file_test_pred_test.write(item+"\n")
# file_test_pred_test.close()

"""## Task 2: Using GloVe word embeddings"""

## building GLOVE word vocab
word_vectors = {}

with gzip.open(glove_file, 
               'rt', encoding='utf8') as f:
    for line in f:
        values = line.split()
        word = values[0]
        
        # idx_to_word[len(idx_to_word)] = word
        vec = np.asarray(values[1:], dtype='float32')
        word_vectors[word] = vec

## function to detect lower case charater in a string
def detectlower(string):
  for char in string:
    if char.islower():
      return True
  return False


def create_w2v(sentences, tagslist):
  text = []
  tags = []


  for sentence in sentences:
    sentence_text = []
    for item in sentence:
      if item.lower() in word_vectors:
        if detectlower(item):
          # value = word_vectors[item].append(1)
          item = item.lower()
          value = np.append(word_vectors[item], 1)
        else:
          item = item.lower()
          value = np.append(word_vectors[item], 0)
      else:
        value = np.zeros(((100,)))
        # value = np.random.normal(size=(100,))
        value = np.append(value, 1)
      sentence_text.append(value)
    text.append(sentence_text)

  for taglst in tagslist:
    sentence_tag = []
    for item in taglst:
      sentence_tag.append(torch.tensor(tag2id[item]))
    tags.append(sentence_tag)      

# print(text)
  text = [torch.tensor(i) for i in text]
  text = pad_sequence(text, batch_first = True, padding_value= 0) 
  tags = [torch.tensor(i) for i in tags]
  tags = pad_sequence(tags, batch_first = True, padding_value= 100)
  return text, tags

def create_w2v_test(sentences):
  text = []


  for sentence in sentences:
    sentence_text = []
    for item in sentence:
      if item.lower() in word_vectors:
        if detectlower(item):
          # value = word_vectors[item].append(1)
          item = item.lower()
          value = np.append(word_vectors[item], 1)
        else:
          item = item.lower()
          value = np.append(word_vectors[item], 0)
      else:
        value = np.zeros(((100,)))
        # value = np.random.normal(size=(100,))
        value = np.append(value, 1)
      sentence_text.append(value)
    text.append(sentence_text)


# print(text)
  text = [torch.tensor(i) for i in text]
  text = pad_sequence(text, batch_first = True, padding_value= 0) 

  return text

train_text, train_tags = create_w2v(sentences, tags_all)
dev_text, dev_tags = create_w2v(sentences_dev, tags_dev)
test_text = create_w2v_test(test_sentence)


# create a TensorDataset
train_dataset = TensorDataset(train_text, train_tags)
dev_dataset = TensorDataset(dev_text, dev_tags)
# create the train_dataloader
train_dataloader2 = DataLoader(train_dataset, batch_size=20, shuffle=True)
dev_dataloader2 = DataLoader(dev_dataset, batch_size=20, shuffle=True)



class GloveModel(nn.Module):
    def __init__(self, num_embeddings, embedding_dim, hidden_dim, output_dim, dropout, num_layers):
    
        super().__init__()
        # self.embedding = nn.Embedding(num_embeddings, embedding_dim)
        self.bilstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, 
                              bidirectional=True, batch_first=True)
        self.dropout = nn.Dropout(dropout)
        self.linear = nn.Linear(hidden_dim * 2, hidden_dim)
        self.elu = nn.ELU()
        # self.classifier = nn.Linear(hidden_dim, output_dim)
        self.classifier = nn.Linear(hidden_dim, output_dim)
        # self.cl2 = nn.Linear(output_dim,9)
        # self.linear = nn.Linear(hidden_dim*2, hidden_dim)
        # self.activation = nn.ELU()
        # self.classifier = nn.Linear(hidden_dim, output_dim)
        self.cl2 = nn.Linear(output_dim, 9)


        
         
    def forward(self, x):
        # out = self.embedding(x)
        out, _ = self.bilstm(x)      
        out = self.dropout(out)   
        out = self.linear(out)      
        out = self.elu(out)    
        out = self.classifier(out)  
        out = self.cl2(out)
        out = out.permute(0,2,1)

        return out

# embedding_dim = 101
# hidden_dim = 256

# num_layers = 1
# dropout = 0.33
# output_dim = 128
# lr = 0.15
# batch_size = 25
# epochs = 140
# # CUDA_LAUNCH_BLOCKING = "1"
# # torch.backends.cudnn.deterministic = True
# # Specify the device (GPU if available, otherwise CPU)

# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# model = GloveModel(num_embeddings=14987, embedding_dim=embedding_dim, 
#                  hidden_dim=hidden_dim, output_dim=output_dim, dropout = dropout,
#                  num_layers = num_layers)


# blstm2_model = model.to(device)
# loss_function = nn.CrossEntropyLoss(ignore_index = 100)
# optimizer = torch.optim.SGD(blstm2_model.parameters(), lr=lr)

# # Train the model
# for epoch in range(epochs):
#     epoch_loss = 0.0
#     for batch_inputs, batch_labels in train_dataloader2:
#         # Send the batch to the device
#         batch_inputs, batch_labels = batch_inputs.to(device), batch_labels.to(device)
#         # print(batch_inputs.size(), batch_labels.size())

#         optimizer.zero_grad()
#         outputs = blstm2_model(batch_inputs.float())

#         # torch.Size([20, 113, 101]) torch.Size([20, 113])
#         # torch.Size([20, 128, 113])
#         # print(outputs.size())
#         loss = loss_function(outputs, batch_labels)

#         loss.backward()
#         optimizer.step()
#         epoch_loss += loss.item()
#         pred = torch.argmax(outputs,axis=1)


#     epoch_loss /= len(train_dataloader2)
#     print(f"Epoch {epoch+1}: Loss={epoch_loss:.4f}")

# ## save model
save_path_blstm2 = save_path + "/blstm2.pt"
# torch.save(blstm2_model, save_path_blstm2)

# #load model
blstm2_model = torch.load(save_path_blstm2, map_location=torch.device("cuda" if torch.cuda.is_available() else "cpu"))
print('model2')

#load model
# save_path_blstm2 = "gdrive/MyDrive/Colab Notebooks/HW4/blstm2.pt"
# blstm2_model = torch.load(save_path_blstm2)

def predictions(dev_x, dev_y, ignore_idx, blstm1):
  true_tag = []
  position = []
  predictions = []
  final_pred = []
  for i in dev_y:
    true_tag+=i[i!=ignore_idx].tolist()
    position.append(i[i!=ignore_idx].tolist())

  dev_dataset = TensorDataset(dev_x)
  dev_d = DataLoader(dev_dataset,shuffle=False)

  blstm1.eval()
  with torch.no_grad():
    for inputs in dev_d:
      i = inputs[0].to(device).float()
        
      outputs = blstm1(i)
      predictions.append(torch.argmax(outputs, dim=1))

  for i in range(len(position)):
    
    final_pred+=predictions[i][0][:len(position[i])].tolist()
  return true_tag, final_pred

## predictions for test data

y_pred_test = []
pop = []
final_predictions = []
ignore_idx = 100
# y_true = []
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
blstm2_model.eval()
test_dataset = TensorDataset(test_text)
test_d = DataLoader(test_dataset,shuffle=False)

with torch.no_grad():
    for batch_inputs in test_d:           
        batch_inputs = batch_inputs[0].to(device).float()

        output = blstm2_model(batch_inputs)

        y_pred_test.append(torch.argmax(output, dim=1))

for i in test_text:
    pop.append(i[i!=ignore_idx].tolist())
# print(len(y_pred_test), len(pop))

for i in range(len(pop)):
    
    final_predictions+=y_pred_test[i][0][:len(pop[i])].tolist()

from sklearn.metrics import f1_score
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
true_tag2, pred2 = predictions(dev_text, dev_tags, 100, blstm2_model)
f1_score2 = f1_score(true_tag2, pred2, average='macro')
print('F1 score in sklearn for model2 is', f1_score2)

f2 = open(input_v, "r")
val_text = [line.strip() for line in f2.readlines()]

j = 0
for i in range(len(val_text)):
  if val_text[i] == '':
    continue
  else:
    split_str = val_text[i].split(' ')
    split_str[2] = id2tag[pred2[j]]
    val_text[i] = ' '.join(split_str)
    j += 1
    # item += '3'

f3 = open(input_t, "r")
test_text = [line.strip() for line in f3.readlines()]
## add predictions to test
j = 0 
for i in range(len(test_text)):
  if test_text[i] == '':
    continue
  else:
    test_text[i] += ' '
    test_text[i] += id2tag[final_predictions[j]]
    j += 1

## self-predicted file
f2 = open(input_v, "r")
val_text_self = [line.strip() for line in f2.readlines()]

j = 0
for i in range(len(val_text_self)):
  if val_text_self[i] == '':
    continue
  else:
    val_text_self[i] += ' '
    # print(pred[j])
    val_text_self[i] += id2tag[pred2[j]]
    j += 1
    # item += '3'

print('output files for model 2')
file_test_pred = open('dev2.out','w')
for item in val_text:
	file_test_pred.write(item+"\n")
file_test_pred.close()

# file_test_pred_self = open('self_dev2.out','w')
# for item in val_text_self:
# 	file_test_pred_self.write(item+"\n")
# file_test_pred_self.close()

# file_test_pred_test = open('test2.out','w')
# for item in test_text:
# 	file_test_pred_test.write(item+"\n")
# file_test_pred_test.close()